{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* RelU Function: ReLU, viết tắt của 'Rectified Linear Unit', là một hàm kích hoạt rất phổ biến trong neural networks. Được đánh giá cao vì tính đơn giản nhưng hiểu quả, ReLU được sử dụng rộng rãi để giúp neural networks học được những đặc trưng phức tạp mà không gặp phải vấn đề biến mất gradient. ReLU hoạt động dựa trên nguyên tắc rất đơn giản: nếu Đầu vào là số dương, hàm sẽ trả về giá trị đó.\n",
    "* Ứng dụng: ReLU phổ biến trong các ứng dụng như nhận dạng hình ảnh và xử lý ngôn ngữ tự nhiên, nơi ReLU giúp cải thiện tốc độ học và giảm thểu vấn đề biến mất gradient. ReLU cũng rất quan trong trong học tăng cường và cc tác vụ phân loại , cung cấp một phương pháp hiệu quả để xử lý thông tin phi tuyến tính.\n",
    "* Ưu điểm: \n",
    "    + Tính toán đơn giản: Do cấu trúc đơn giản, ReLU nhanh và hiểu quả hơn trong việc tính toán so với các hàm kích hoạt phi tuyến tính khác \n",
    "    + Giảm mất mát gradient: ReLU giúp giảm thiểu vấn đề biến mất gradient, một điểm mạnh quan trọng trong quá trình huấn luyện neural networks\n",
    "* Nhược điểm:\n",
    "    + Vấn đề Dying ReLU: Đôi khi neuron có thể chỉ trả về giá trị 0 cho tất cả các đầu vào, dẫn đến hiện tượng \"dying ReLU\", khi đó neuron trở nên không hoạt động.\n",
    "    + Không đối xứng tại zero: Do ReLU không phải là hàm có giá trị trung bình bằng 0 nên c thể gây ra vấn đề trong quá trình tối ưu hóa mạng\n",
    "\n",
    "> relu(x) = $\\{_{0 \\if x \\le 0}^{x \\if x \\gt 0} $"
   ],
   "id": "2abb9cf6e28ad621"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T07:09:16.468874Z",
     "start_time": "2024-06-05T07:09:16.457857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def relu( x ):\n",
    "\t\"\"\"\n",
    "    Hàm mô phỏng ReLU Function.\n",
    "  \n",
    "    Args:\n",
    "      x: Giá trị đầu vào.\n",
    "  \n",
    "    Returns:\n",
    "      Giá trị đầu ra sau khi áp dụng hàm ReLU.\n",
    "    \"\"\"\n",
    "\treturn max(0, x)\n",
    "\n",
    "def main():\n",
    "\trandom_list = []\n",
    "\tn = 100\n",
    "\tl = 0\n",
    "\tr = 1\n",
    "\tfor _ in range(n):\n",
    "\t\trandom_num = random.randint(l, r)\n",
    "\t\trandom_list.append(random_num)\n",
    "\tprint([relu(i) for i in random_list])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ],
   "id": "4ea70c6ef954200e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4e701602c9c533e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
